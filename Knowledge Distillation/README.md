## Main Notebook Files
This project demonstrates the use and impact of Knowledge Distillation (KD) with the MNIST dataset using TensorFlow. KD is important for taking a large resource intensive Neural Net model and condensing it into a smaller model for IoT or end device applications while attempting to maintain a high level of accuracy.

### Task1.ipynb
This notebook contains many sections that must be run from top to bottom to get the results of KD on the MNIST dataset.

## References to Open Source Libraries
- F. Pedregosa et al., “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res., vol. 12, no. 85, pp. 2825–2830, 2011.
- J. D. Hunter, “Matplotlib: A 2D Graphics Environment,” Comput. Sci. Eng., vol. 9, no. 3, pp. 90–95, May 2007, doi: 10.1109/MCSE.2007.55.
- M. Abadi et al., “TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,” ArXiv160304467 Cs, Mar. 2016, Accessed: Mar. 01, 2022. [Online]. Available: http://arxiv.org/abs/1603.04467
- M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, USA, Aug. 2016, pp. 1135–1144. doi: 10.1145/2939672.2939778.
- “Keras: the Python deep learning API.” https://keras.io/ (accessed Mar. 01, 2022).
- C. R. Harris et al., “Array programming with NumPy,” Nature, vol. 585, no. 7825, pp. 357–362, Sep. 2020, doi: 10.1038/s41586-020-2649-2.
