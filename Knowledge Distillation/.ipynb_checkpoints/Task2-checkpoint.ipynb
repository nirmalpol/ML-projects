{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccede27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Flatten\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from tensorflow.core.util import event_pb2\n",
    "from tensorflow.python.lib.io import tf_record\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import csv\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "972384de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nirmal/Downloads/Project_B_Supp/mhist_dataset/mhist_dataset/HMT_train/01_SSA\n"
     ]
    }
   ],
   "source": [
    "train_path_01_SSA = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_train', '01_SSA')\n",
    "train_path_02_HP = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_train', '02_HP')\n",
    "\n",
    "test_path_01_SSA = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_test', '01_SSA')\n",
    "test_path_02_HP = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_test', '02_HP')\n",
    "\n",
    "#print(train_path_01_SSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee24051",
   "metadata": {},
   "source": [
    "# Preprocessing the data into train and test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f581df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "annotation_path = os.path.join(os.getcwd(), 'mhist_dataset', 'annotations.csv')\n",
    "with open(annotation_path, 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    for row in csvreader:\n",
    "        filePath = os.path.join(os.getcwd(), 'mhist_dataset', 'images', row[0])\n",
    "        curr_image = os.path.join(os.getcwd(), 'mhist_dataset', 'images')\n",
    "        \n",
    "        if row[3] == 'train' and row[1] == 'SSA':\n",
    "            shutil.copy(filePath, train_path_01_SSA)\n",
    "        elif row[3] == 'train' and row[1] == 'HP':\n",
    "            shutil.copy(filePath, train_path_02_HP)\n",
    "            \n",
    "        elif row[3] == 'test' and row[1] == 'SSA':\n",
    "            shutil.copy(filePath, test_path_01_SSA)\n",
    "        elif row[3] == 'test' and row[1] == 'HP':\n",
    "            shutil.copy(filePath, test_path_02_HP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3c68bef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2175 images belonging to 2 classes.\n",
      "Found 977 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reusing datagenerator from Project A and applying same data augmentations as Project A\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_train') #you should change to your directory\n",
    "test_dir = os.path.join(os.getcwd(), 'mhist_dataset', 'mhist_dataset', 'HMT_test') #you should change to your directory\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.,\n",
    "shear_range=0.1,\n",
    "rotation_range=15,\n",
    "horizontal_flip=True,\n",
    "vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "class_mode='categorical',\n",
    "interpolation='bilinear',\n",
    "target_size=(224, 224),\n",
    "batch_size=32,\n",
    "shuffle=True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "class_mode='categorical',\n",
    "interpolation='bilinear',\n",
    "target_size=(224, 224),\n",
    "batch_size=32,\n",
    "shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b5560",
   "metadata": {},
   "source": [
    "# Creating Teacher & Student Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac98459",
   "metadata": {},
   "source": [
    "## Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8278924e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "\n",
    "resnet = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    ")\n",
    "\n",
    "output = resnet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "teacher_model = Model(resnet.input, output)\n",
    "\n",
    "teacher_model.trainable = False\n",
    "\n",
    "teacher = Sequential()\n",
    "teacher.add(teacher_model)\n",
    "teacher.add(Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0fa2d",
   "metadata": {},
   "source": [
    "## Student with KD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d61bd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "output = mobilenet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "mobile_model = Model(mobilenet.input, output)\n",
    "\n",
    "mobile_model.trainable = False\n",
    "\n",
    "student_KD = Sequential()\n",
    "student_KD.add(mobile_model)\n",
    "student_KD.add(Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae37136",
   "metadata": {},
   "source": [
    "## Student without KD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9dd1de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "output = mobilenet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "mobile_model = Model(mobilenet.input, output)\n",
    "\n",
    "mobile_model.trainable = False\n",
    "\n",
    "student_noKD = Sequential()\n",
    "student_noKD.add(mobile_model)\n",
    "student_noKD.add(Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc12072",
   "metadata": {},
   "source": [
    "# Creating Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e447aff",
   "metadata": {
    "id": "8JWGucyrQGav"
   },
   "source": [
    "## Teacher loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "52557200",
   "metadata": {
    "id": "DhzBP6ZLQJ57"
   },
   "outputs": [],
   "source": [
    "def compute_teacher_loss(images, labels):\n",
    "  # REUSING TASK 1 CODE\n",
    "    \n",
    "  #Compute the subclass_logits and apply cross entropy on the logits of the model.\n",
    "  subclass_logits = teacher(images, training=True)\n",
    "\n",
    "  #The model was built without softmax so the last layer or output will be the logits.\n",
    "  cross_entropy_for_batch = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
    "\n",
    "  #Since this is a batch of images, we need to average the loss and return it.\n",
    "  cross_entropy_loss_value = tf.reduce_mean(cross_entropy_for_batch)\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41469b",
   "metadata": {
    "id": "JS8xkuH0QbOS"
   },
   "source": [
    "## Student with KD loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "84ba2647",
   "metadata": {
    "id": "lDKia4gPQMIr"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "# Hyperparameters for distillation (need to be tuned).\n",
    "ALPHA = 0.5 # task balance between cross-entropy and distillation loss\n",
    "DISTILLATION_TEMPERATURE = 4. #temperature hyperparameter\n",
    "\n",
    "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
    "                      temperature: Union[float, tf.Tensor]):\n",
    "\n",
    "  # RESUSING TASK 1 CODE\n",
    "\n",
    "  soft_targets = tf.nn.softmax(teacher_logits/temperature)\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
    "\n",
    "def compute_student_loss(images, labels):\n",
    "  # RESUSING TASK 1 CODE\n",
    "\n",
    "  student_subclass_logits = student_KD(images, training=True)\n",
    "  teacher_subclass_logits = teacher(images, training=False)\n",
    "  distillation_loss_value = distillation_loss(teacher_subclass_logits, student_subclass_logits, DISTILLATION_TEMPERATURE)\n",
    "\n",
    "  cross_entropy_loss_batch = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=student_subclass_logits)\n",
    "  hard_cross_entropy_loss = tf.reduce_mean(cross_entropy_loss_batch)\n",
    "\n",
    "  cross_entropy_loss_value = ALPHA*hard_cross_entropy_loss + (1-ALPHA)*distillation_loss_value\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35983223",
   "metadata": {},
   "source": [
    "## Student without KD loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "af3cdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_student_noKD_loss(images, labels):\n",
    "  # REUSING TASK 1 CODE\n",
    "    \n",
    "  #Compute the subclass_logits and apply cross entropy on the logits of the model.\n",
    "  subclass_logits = student_noKD(images, training=True)\n",
    "\n",
    "  #The model was built without softmax so the last layer or output will be the logits.\n",
    "  cross_entropy_for_batch = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
    "\n",
    "  #Since this is a batch of images, we need to average the loss and return it.\n",
    "  cross_entropy_loss_value = tf.reduce_mean(cross_entropy_for_batch)\n",
    "\n",
    "  return cross_entropy_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9045ddb",
   "metadata": {
    "id": "RJ1uyvurQ3w4"
   },
   "source": [
    "## Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "928109a8",
   "metadata": {
    "id": "EtoLbp8uQ4Vl"
   },
   "outputs": [],
   "source": [
    "logits = 0\n",
    "@tf.function\n",
    "def compute_num_correct(model, images, labels):\n",
    "  # REUSING TASK 1 CODE\n",
    "  class_logits = model(images, training=False)\n",
    "  #print(tf.argmax(labels, -1))\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
    "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, compute_loss_fn, l_rate):\n",
    "  # REUSING TASK 1 CODE\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=l_rate)\n",
    "  \n",
    "    \n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "    # Run training.\n",
    "    print('Epoch {}: '.format(epoch), end='')\n",
    "    for i in range(len(train_generator)):\n",
    "          images, labels = train_generator[i]\n",
    "          #print(len(labels))\n",
    "          #print(len(images))\n",
    "          #print(i)\n",
    "          with tf.GradientTape() as tape:\n",
    "            loss_value = compute_loss_fn(images, labels)\n",
    "\n",
    "          # Obtained from tensorflow documentation\n",
    "          grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Run evaluation.\n",
    "    num_correct = 0\n",
    "    num_total = len(test_generator)*32\n",
    "    for i in range(len(test_generator)):\n",
    "        images, labels = test_generator[i] \n",
    "        #print(compute_num_correct(model, images, labels)[0]) #Prints sum of correct answers\n",
    "        #print(compute_num_correct(model, images, labels)[1]) #Numbers that were predicted correctly\n",
    "        num_correct += compute_num_correct(model, images, labels)[0]\n",
    "        \n",
    "        \n",
    "        # The logits output do not make sense as they do not map well with the labels very often\n",
    "        # I suspect that there is an issue with the implementation of the model itself.\n",
    "        \n",
    "        #logits = model(images, training=False)\n",
    "        #print(logits)\n",
    "        #print(labels)\n",
    "        #from sklearn.metrics import precision_score\n",
    "        #precision = precision_score(labels, logits, labels=[0,1], average='micro')\n",
    "        #print(\"The precision on the test set is \" + str(precision))\n",
    "\n",
    "        #from sklearn.metrics import recall_score\n",
    "        #recall = recall_score(labels, logits, labels=[0,1], average='micro')\n",
    "        #print(\"The recall on the test set is \"+str(recall))\n",
    "\n",
    "        #f_1 = (2 * precision * recall) / (precision + recall)\n",
    "        #print(\"The F-1 score on the test set is \"+str(f_1))\n",
    "        \n",
    "    print(\"Class_accuracy: \" + '{:.2f}%'.format(\n",
    "            num_correct / num_total * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaf82d",
   "metadata": {
    "id": "NQL1lJdaRPT1"
   },
   "source": [
    "## Training and Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2b86d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 39s 1s/step - loss: 0.7045 - accuracy: 0.7257\n"
     ]
    }
   ],
   "source": [
    "teacher.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "loss, acc = teacher.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fd0e04cc",
   "metadata": {
    "id": "-AGHbyABRPz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the Teacher Model\n",
      "Epoch 0: Class_accuracy: 71.47%\n",
      "Epoch 1: Class_accuracy: 76.31%\n",
      "Epoch 2: Class_accuracy: 77.22%\n",
      "Epoch 3: Class_accuracy: 77.32%\n",
      "Epoch 4: Class_accuracy: 73.39%\n",
      "Epoch 5: Class_accuracy: 73.39%\n",
      "Epoch 6: Class_accuracy: 76.11%\n",
      "Epoch 7: Class_accuracy: 74.90%\n",
      "Epoch 8: Class_accuracy: 77.12%\n",
      "Epoch 9: Class_accuracy: 73.89%\n",
      "Testing the Student with KD Model\n",
      "Epoch 0: Class_accuracy: 73.59%\n",
      "Epoch 1: Class_accuracy: 64.82%\n",
      "Epoch 2: Class_accuracy: 68.75%\n",
      "Epoch 3: Class_accuracy: 68.15%\n",
      "Epoch 4: Class_accuracy: 68.45%\n",
      "Epoch 5: Class_accuracy: 74.29%\n",
      "Epoch 6: Class_accuracy: 71.47%\n",
      "Epoch 7: Class_accuracy: 70.77%\n",
      "Epoch 8: Class_accuracy: 72.68%\n",
      "Epoch 9: Class_accuracy: 72.58%\n",
      "Testing the Student without KD Model\n",
      "Epoch 0: Class_accuracy: 72.68%\n",
      "Epoch 1: Class_accuracy: 72.38%\n",
      "Epoch 2: Class_accuracy: 73.89%\n",
      "Epoch 3: Class_accuracy: 75.30%\n",
      "Epoch 4: Class_accuracy: 75.71%\n",
      "Epoch 5: Class_accuracy: 74.50%\n",
      "Epoch 6: Class_accuracy: 74.70%\n",
      "Epoch 7: Class_accuracy: 72.88%\n",
      "Epoch 8: Class_accuracy: 74.70%\n",
      "Epoch 9: Class_accuracy: 73.79%\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing for Teacher Model.\n",
    "NUM_EPOCHS=10\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "print(\"Testing the Teacher Model\")\n",
    "train_and_evaluate(teacher, compute_teacher_loss, l_rate=1e-4)\n",
    "\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "print(\"Testing the Student with KD Model\")\n",
    "train_and_evaluate(student_KD, compute_student_loss, l_rate=1e-3)\n",
    "\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "print(\"Testing the Student without KD Model\")\n",
    "train_and_evaluate(student_noKD, compute_student_noKD_loss, l_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fd5f6",
   "metadata": {},
   "source": [
    "# Comparing the Teacher and Student models (number of of parameters and FLOPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7c6e5b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:5214: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "Teacher Model: \n",
      "FLOPS: 2.24e+02 G\n",
      "Student Model with KD: \n",
      "FLOPS: 19.6 G\n",
      "Student Model without KD: \n",
      "FLOPS: 19.6 G\n"
     ]
    }
   ],
   "source": [
    "# your code start from here for step 8\n",
    "'''\n",
    "#The following FLOPS code obtained from Keras-Flops:\n",
    "/***************************************************************************************\n",
    "*    Title: FLOPs calculator for neural network architecture written in tensorflow\n",
    "*    Author: tokusumi\n",
    "*    Date: August 17, 2020\n",
    "*    Code version: N/A\n",
    "*    Availability: https://github.com/tokusumi/keras-flops\n",
    "*\n",
    "***************************************************************************************/\n",
    "'''\n",
    "from keras_flops import get_flops\n",
    "flops = get_flops(teacher, batch_size=32)\n",
    "print(\"Teacher Model: \")\n",
    "print(f\"FLOPS: {flops / 10**9:.03} G\")\n",
    "\n",
    "flops = get_flops(student_KD, batch_size=32)\n",
    "print(\"Student Model with KD: \")\n",
    "print(f\"FLOPS: {flops / 10**9:.03} G\")\n",
    "\n",
    "flops = get_flops(student_noKD, batch_size=32)\n",
    "print(\"Student Model without KD: \")\n",
    "print(f\"FLOPS: {flops / 10**9:.03} G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c10c53",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Number of layers in the base model: \", len(teacher_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "#fine_tune_at = 180\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "#for layer in teacher_model.layers[:fine_tune_at]:\n",
    "#  layer.trainable = False\n",
    "\n",
    "#import pandas as pd\n",
    "#pd.set_option('max_colwidth', -1)\n",
    "#layers = [(layer, layer.name, layer.trainable) for layer in teacher_model.layers]\n",
    "#pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
